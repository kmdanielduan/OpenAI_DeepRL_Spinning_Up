# OpenAI Spinning Up

## 1. Key Concepts in RL

URL: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html

### Key Concepts and Terminology

![rl_diagram_transparent_bg](assets/rl_diagram_transparent_bg.png)

The main characters of RL are the **agent** and the **environment**. The environment is the world that the agent lives in and interacts with.

The agent also perceives a **reward** signal from the environment, a number that tells it how good or bad the current world state is.

The goal of the agent is to maximize its cumulative reward, called **return**.

- states and observations,
- action spaces,
- policies,
- trajectories,
- different formulations of return,
- the RL optimization problem,
- and value functions.

#### States and Observations

- **state** $s$ : a complete description of the state of the world.

- **observation** $o$ : a partial description of a state, which may omit information.

#### Action Spces

- **action space**: The set of all valid actions in a given environment
  - **Discrete action spaces**
  - **Continuous action spaces**

#### Policies

- **policy**: a rule used by an agent to decide what actions to take.
  - **deterministic**: $a_t=\mu(s_t)$
  - **stochastic**: $a_t=\pi( \cdot |s_t)$
- In deep RL, we deal with **parameterized policies**: policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm.
  - **deterministic**: $a_t=\mu_\theta(s_t)$
  - **stochastic**: $a_t=\pi_\theta( \cdot |s_t)$

#### Deterministic Policies

**Sample:**

```
obs = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)
net = mlp(obs, hidden_dims=(64,64), activation=tf.tanh)
actions = tf.layers.dense(net, units=act_dim, activation=None)
```

#### Stochastic Policies

The two most common kinds of stochastic policies in deep RL are **categorical policies** and **diagonal Gaussian policies**.

Two key computations are centrally important for using and training stochastic policies:

- sampling actions from the policy,
- and computing log likelihoods of particular actions, $\log\pi_\theta(a|s)$.

- **Categorical policies** for discrete actions spaces.
  - **Sampling.** Given the probabilities for each action, frameworks like Tensorflow have built-in tools for sampling.
  - **Log-Likelihood.** Denote the last layer of probabilities as ![P_{\theta}(assets/979f389d6361570ecffa9bd20203ebc38e954a73.svg)](https://spinningup.openai.com/en/latest/_images/math/979f389d6361570ecffa9bd20203ebc38e954a73.svg). It is a vector with however many entries as there are actions, so we can treat the actions as indices for the vector. The log likelihood for an action ![a](assets/045c129c396240f9c4f59bee864337cea3896ef0.svg) can then be obtained by indexing into the vector:![57917ab8193871a92f1ec1f40756950dd7b483fb](assets/57917ab8193871a92f1ec1f40756950dd7b483fb.svg)

- **Diagonal Gaussian policies** for continuous action spaces.

  - A multivariate Gaussian distribution (or multivariate normal distribution, if you prefer) is described by a mean vector, ![\mu](assets/0e287adf9726dd38024f3426abea7e2c0b7b2fc9.svg), and a covariance matrix, ![\Sigma](assets/0628d30db5aa0717a34042ffa192db6e7753520a.svg). A diagonal Gaussian distribution is a special case where the covariance matrix only has entries on the **diagonal**. As a result, we can represent it by a vector.

  - A diagonal Gaussian policy always has a neural network that maps from observations to mean actions, ![\mu_{\theta}(assets/63256a562afdee2aeadc5677ccac33e10896a289.svg)](https://spinningup.openai.com/en/latest/_images/math/63256a562afdee2aeadc5677ccac33e10896a289.svg). There are two different ways that the covariance matrix is typically represented.

    1. There is a single vector of log standard deviations, ![\log \sigma](assets/898f05e373e384546329d1f1d4840a3bd0f23c10.svg), which is **not** a function of state: the ![\log \sigma](assets/898f05e373e384546329d1f1d4840a3bd0f23c10.svg) are standalone parameters. (You Should Know: our implementations of VPG, TRPO, and PPO do it this way.)
    2. There is a neural network that maps from states to log standard deviations, ![\log \sigma_{\theta}(assets/0a3da071d616ab9afab7dbe90b3245313a7d3130.svg)](https://spinningup.openai.com/en/latest/_images/math/0a3da071d616ab9afab7dbe90b3245313a7d3130.svg). It may optionally share some layers with the mean network.

    - Note: use log standard deviations instead of standard deviations directly. Because log stds are free to take on any values in ![(assets/ed0a20683a06f2589b44e531d41c359ad40bee4f.svg)](https://spinningup.openai.com/en/latest/_images/math/ed0a20683a06f2589b44e531d41c359ad40bee4f.svg), while stds must be nonnegative.

  - **Sampling.** Given the mean action ![\mu_{\theta}(assets/63256a562afdee2aeadc5677ccac33e10896a289-20190613212822952.svg)](https://spinningup.openai.com/en/latest/_images/math/63256a562afdee2aeadc5677ccac33e10896a289.svg) and standard deviation ![\sigma_{\theta}(assets/7cee230e6ebcfed3a3ec8ff724899f6fe4c123c7.svg)](https://spinningup.openai.com/en/latest/_images/math/7cee230e6ebcfed3a3ec8ff724899f6fe4c123c7.svg), and a vector ![z](assets/9df411616324d64a02f28dedff6aacf05d2bf1fd.svg) of noise from a spherical Gaussian (![z \sim \mathcal{N}(assets/f8811e4365bceff49b2667b4b3eec29c3d1567f6.svg)](https://spinningup.openai.com/en/latest/_images/math/f8811e4365bceff49b2667b4b3eec29c3d1567f6.svg)), an action sample can be computed with

    ![a = \mu_{\theta}(assets/99fb04c4b47e1e6d6e05b939d9f648f6920e3754.svg) + \sigma_{\theta}(s) \odot z,](https://spinningup.openai.com/en/latest/_images/math/99fb04c4b47e1e6d6e05b939d9f648f6920e3754.svg)

    where ![\odot](assets/8a1c8ebce519d2342df611fb6df18da32baac216.svg) denotes the elementwise product of two vectors. 

  - **Log-Likelihood.** The log-likelihood of a ![k](assets/ce641c655353b854e8d2ae836c6f97df9ace9984.svg) -dimensional action ![a](assets/045c129c396240f9c4f59bee864337cea3896ef0-20190613213021789.svg), for a diagonal Gaussian with mean ![\mu = \mu_{\theta}(assets/d8151ea477dc9219492447408ee17f2e8cb3a4b7.svg)](https://spinningup.openai.com/en/latest/_images/math/d8151ea477dc9219492447408ee17f2e8cb3a4b7.svg) and standard deviation ![\sigma = \sigma_{\theta}(assets/09a783f3fb9e8ed1aef0ca44a810e67d09a52a4f.svg)](https://spinningup.openai.com/en/latest/_images/math/09a783f3fb9e8ed1aef0ca44a810e67d09a52a4f.svg), is given by

$$
\pi_\theta(a_i|s)=\frac{1}{\sqrt{2\pi \sigma_i^2}}e^{-\frac{(x-\mu_i)^2}{2\sigma^2}}
$$

​											![\log \pi_{\theta}(assets/003c0eae9ef9660bf067815175ddd51b487c5191.svg) = -\frac{1}{2}\left(\sum_{i=1}^k \left(\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \log \sigma_i \right) + k \log 2\pi \right).](https://spinningup.openai.com/en/latest/_images/math/003c0eae9ef9660bf067815175ddd51b487c5191.svg)

#### Trajectories

A trajectory ![\tau](assets/018b60fd9e01bf256b27f2b67a2399df216eb7b2.svg) is a sequence of states and actions in the world,

![\tau = (assets/c93cc4ca90e2a21ee0e4b60c6ee51b422a73b84c.svg).](https://spinningup.openai.com/en/latest/_images/math/c93cc4ca90e2a21ee0e4b60c6ee51b422a73b84c.svg)

The very first state of the world, ![s_0](assets/3dab911480e6ed406b73a47513b25e6f897875e4.svg), is randomly sampled from the **start-state distribution**, sometimes denoted by ![\rho_0](assets/b9c31eb35f400fd8e50b2d39bfe5698a50b9b6b5.svg):

![s_0 \sim \rho_0(assets/b1af7acc54e45820e1996fcd53b56681ced155b4.svg).](https://spinningup.openai.com/en/latest/_images/math/b1af7acc54e45820e1996fcd53b56681ced155b4.svg)

State transitions (what happens to the world between the state at time ![t](assets/53e1d388a3a49063e3444d86c79fd887cc3d5b6c.svg), ![s_t](assets/8b415ee5fdf45796473d9a05615664c67e20af3d.svg), and the state at ![t+1](assets/fa05960e3f62f777452c6a00e817772565aabd47.svg), ![s_{t+1}](assets/eddbb0a5b1f95b8e4a2d25b156ec0f61c24dfe41.svg)), are governed by the natural laws of the environment, and depend on only the most recent action, ![a_t](assets/8ddb1f258c0be380f1821db82450a8c1ed8bf138.svg). They can be either deterministic,

![s_{t+1} = f(assets/5c77b8a5172ba1b6812a2f190eb1d9a4388b16b9.svg)](https://spinningup.openai.com/en/latest/_images/math/5c77b8a5172ba1b6812a2f190eb1d9a4388b16b9.svg)

or stochastic,

![s_{t+1} \sim P(assets/ab9d75a3d467228b0b7cba14c4c6368c7b58f546.svg).](https://spinningup.openai.com/en/latest/_images/math/ab9d75a3d467228b0b7cba14c4c6368c7b58f546.svg)

Actions come from an agent according to its policy.

#### Reward and Return

The reward function ![R](assets/d110c31079c0f9249b74f4bb82c84fd93ea461e4.svg) depends on the current state of the world, the action just taken, and the next state of the world:

![r_t = R(assets/b298a591c3c7a428a5b98fe92279b5b8d1b8eb96.svg)](https://spinningup.openai.com/en/latest/_images/math/b298a591c3c7a428a5b98fe92279b5b8d1b8eb96.svg)

although frequently this is simplified to just a dependence on the current state, ![r_t = R(assets/b59b3980c45941c21ab5da6fb9f921dd2d669d5f.svg)](https://spinningup.openai.com/en/latest/_images/math/b59b3980c45941c21ab5da6fb9f921dd2d669d5f.svg), or state-action pair ![r_t = R(assets/2c5b7830b53ea578c05512cd80de5749f045a237.svg)](https://spinningup.openai.com/en/latest/_images/math/2c5b7830b53ea578c05512cd80de5749f045a237.svg).

The goal of the agent is to maximize some notion of cumulative reward over a trajectory.

- One kind of return is the **finite-horizon undiscounted return**, which is just the sum of rewards obtained in a fixed window of steps:

  ![R(assets/ce20ca1d911ea7b3b9161000c52ed750ec75cc14.svg) = \sum_{t=0}^T r_t.](https://spinningup.openai.com/en/latest/_images/math/ce20ca1d911ea7b3b9161000c52ed750ec75cc14.svg)

  Another kind of return is the **infinite-horizon discounted return**, which is the sum of all rewards *ever* obtained by the agent, but discounted by how far off in the future they’re obtained. This formulation of reward includes a discount factor ![\gamma \in (assets/202f89dbc5e387cec1af4cf1be6e6c87bbd8304e.svg)](https://spinningup.openai.com/en/latest/_images/math/202f89dbc5e387cec1af4cf1be6e6c87bbd8304e.svg):

  ![R(assets/196d139b5647c5a777ebe6ddfce278f8b0736156.svg) = \sum_{t=0}^{\infty} \gamma^t r_t.](https://spinningup.openai.com/en/latest/_images/math/196d139b5647c5a777ebe6ddfce278f8b0736156.svg)

#### The RL Problem

The goal in RL is to select a policy which maximizes **expected return** when the agent acts according to it.

Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a ![T](assets/813bf97f00fe14c6142e061de00769d99ccddf0e.svg) -step trajectory is:

![P(assets/06c47618f62ff44b438c4901b69bb44cba239153.svg) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t).](https://spinningup.openai.com/en/latest/_images/math/06c47618f62ff44b438c4901b69bb44cba239153.svg)

The expected return (for whichever measure), denoted by ![J(assets/e44ba0152f26f27382a11a95f24c132f0d3410a2.svg)](https://spinningup.openai.com/en/latest/_images/math/e44ba0152f26f27382a11a95f24c132f0d3410a2.svg), is then:

![J(assets/cf40a268eae7267d76231420eaf1e143eb870fdc.svg) = \int_{\tau} P(\tau|\pi) R(\tau) = \underE{\tau\sim \pi}{R(\tau)}.](https://spinningup.openai.com/en/latest/_images/math/cf40a268eae7267d76231420eaf1e143eb870fdc.svg)

The central optimization problem in RL can then be expressed by

![\pi^* = \arg \max_{\pi} J(assets/c2a14ef49dc9ccc554528b239896ef4f8afefe37.svg),](https://spinningup.openai.com/en/latest/_images/math/c2a14ef49dc9ccc554528b239896ef4f8afefe37.svg)

with ![\pi^*](assets/3b90d2c0187600be023084b475e921927a91aa4f.svg) being the **optimal policy**.

#### Value Functions

It’s often useful to know the **value** of a state, or state-action pair. By value, we mean the expected return if you start in that state or state-action pair, and then act according to a particular policy forever after. **Value functions** are used, one way or another, in almost every RL algorithm.

There are four main functions of note here.

1. The **On-Policy Value Function**, ![V^{\pi}(assets/3c343749cc2f7804a670b618d03d056d7e35b5eb.svg)](https://spinningup.openai.com/en/latest/_images/math/3c343749cc2f7804a670b618d03d056d7e35b5eb.svg), which gives the expected return if you start in state ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309.svg) and always act according to policy ![\pi](assets/b8a498bfdeca84b973d6222e08a6d04321d65dc3.svg):

   > ![V^{\pi}(assets/f31d6113b295f2a1547b4dbef3f172bf8731070b.svg) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}](https://spinningup.openai.com/en/latest/_images/math/f31d6113b295f2a1547b4dbef3f172bf8731070b.svg)

2. The **On-Policy Action-Value Function**, ![Q^{\pi}(assets/be616d6b2f96aeb2f5c3321d153a30589c80004f.svg)](https://spinningup.openai.com/en/latest/_images/math/be616d6b2f96aeb2f5c3321d153a30589c80004f.svg), which gives the expected return if you start in state ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309.svg), take an arbitrary action ![a](assets/045c129c396240f9c4f59bee864337cea3896ef0-20190613220534055.svg) (which may not have come from the policy), and then forever after act according to policy ![\pi](assets/b8a498bfdeca84b973d6222e08a6d04321d65dc3.svg):

   > ![Q^{\pi}(assets/350596a2cc2f9338f114972639f0d88a77f4b942.svg) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}](https://spinningup.openai.com/en/latest/_images/math/350596a2cc2f9338f114972639f0d88a77f4b942.svg)

3. The **Optimal Value Function**, ![V^*(assets/3aa5fd189ef621bb055e7064dfc5ee24099bc95a.svg)](https://spinningup.openai.com/en/latest/_images/math/3aa5fd189ef621bb055e7064dfc5ee24099bc95a.svg), which gives the expected return if you start in state ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309.svg) and always act according to the *optimal* policy in the environment:

   > ![V^*(assets/bea57d358e59dfc6c11d00961ae55506c0ab0fe8.svg) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}](https://spinningup.openai.com/en/latest/_images/math/bea57d358e59dfc6c11d00961ae55506c0ab0fe8.svg)

4. The **Optimal Action-Value Function**, ![Q^*(assets/3a641ac6d7de30796ff8c186c31692dd609123a0.svg)](https://spinningup.openai.com/en/latest/_images/math/3a641ac6d7de30796ff8c186c31692dd609123a0.svg), which gives the expected return if you start in state ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309.svg), take an arbitrary action ![a](assets/045c129c396240f9c4f59bee864337cea3896ef0-20190613220534055.svg), and then forever after act according to the *optimal* policy in the environment:

   > ![Q^*(assets/322f38074bc53060ff4a39d0abb046e77bf77aae.svg) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}](https://spinningup.openai.com/en/latest/_images/math/322f38074bc53060ff4a39d0abb046e77bf77aae.svg)

- Note: There are two key connections between the value function and the action-value function that come up pretty often:

  - ![V^{\pi}(assets/b2d90201f072965337bacb822b17d8f2e9791165.svg) = \underE{a\sim \pi}{Q^{\pi}(s,a)},](https://spinningup.openai.com/en/latest/_images/math/b2d90201f072965337bacb822b17d8f2e9791165.svg)
    - 

  - ![V^*(assets/45dfe72ba680d985e158c2fef08ddfbb9c5f57a6.svg) = \max_a Q^* (s,a).](https://spinningup.openai.com/en/latest/_images/math/45dfe72ba680d985e158c2fef08ddfbb9c5f57a6.svg)

#### The Optimal Q-Function and the Optimal Action

There is an important connection between the optimal action-value function ![Q^*(assets/3a641ac6d7de30796ff8c186c31692dd609123a0-20190613221121529.svg)](https://spinningup.openai.com/en/latest/_images/math/3a641ac6d7de30796ff8c186c31692dd609123a0.svg) and the action selected by the optimal policy. By definition, ![Q^*(assets/3a641ac6d7de30796ff8c186c31692dd609123a0-20190613221121529.svg)](https://spinningup.openai.com/en/latest/_images/math/3a641ac6d7de30796ff8c186c31692dd609123a0.svg) gives the expected return for starting in state ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309-20190613221121709.svg), taking (arbitrary) action ![a](assets/045c129c396240f9c4f59bee864337cea3896ef0-20190613221121873.svg), and then acting according to the optimal policy forever after.

The optimal policy in ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309-20190613221121709.svg) will select whichever action maximizes the expected return from starting in ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309-20190613221121709.svg). As a result, if we have ![Q^*](assets/27e0f032982b38571547cc19a81946ebdb69e1b9.svg), we can directly obtain the optimal action, ![a^*(assets/217119cd0be21cd4d89636871a724e859a18c54d.svg)](https://spinningup.openai.com/en/latest/_images/math/217119cd0be21cd4d89636871a724e859a18c54d.svg), via

![a^*(assets/4a5e5d2aad03e229c014d1990a78732de2144b9a.svg) = \arg \max_a Q^* (s,a).](https://spinningup.openai.com/en/latest/_images/math/4a5e5d2aad03e229c014d1990a78732de2144b9a.svg)

Note: there may be multiple actions which maximize ![Q^*(assets/3a641ac6d7de30796ff8c186c31692dd609123a0-20190613221121529.svg)](https://spinningup.openai.com/en/latest/_images/math/3a641ac6d7de30796ff8c186c31692dd609123a0.svg), in which case, all of them are optimal, and the optimal policy may randomly select any of them. But there is always an optimal policy which deterministically selects an action.

### Bellman Equations

All four of the value functions obey special self-consistency equations called **Bellman equations**. The basic idea behind the Bellman equations is this:

> The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.

The Bellman equations for the on-policy value functions are

![\begin{align*} V^{\pi}(assets/79ebcb56fd4e6ab3dacd97ecf292f20c65e52f40.svg) &= \underE{a \sim \pi \\ s'\sim P}{r(s,a) + \gamma V^{\pi}(s')}, \\ Q^{\pi}(s,a) &= \underE{s'\sim P}{r(s,a) + \gamma \underE{a'\sim \pi}{Q^{\pi}(s',a')}}, \end{align*}](https://spinningup.openai.com/en/latest/_images/math/79ebcb56fd4e6ab3dacd97ecf292f20c65e52f40.svg)

where ![s' \sim P](assets/ad6bbdc678bfb97bf238df2294b123d8e4659211.svg) is shorthand for ![s' \sim P(assets/b13ab0c59dbdf2e621eab45c58c04468828b298c.svg)](https://spinningup.openai.com/en/latest/_images/math/b13ab0c59dbdf2e621eab45c58c04468828b298c.svg), indicating that the next state ![s'](assets/e571b6cfaee799ef1b1226dab6f2b72a093cd7a7.svg) is sampled from the environment’s transition rules; ![a \sim \pi](assets/208a0418872581e171aa0da9db2ff5fa19e482b7.svg) is shorthand for ![a \sim \pi(assets/5359fa9db8fc409d8a93635b92a2b0a8e8b8f28d.svg)](https://spinningup.openai.com/en/latest/_images/math/5359fa9db8fc409d8a93635b92a2b0a8e8b8f28d.svg); and ![a' \sim \pi](assets/44d76c7c381bf7cbacb231ea9bf27b72f006c0b4.svg) is shorthand for ![a' \sim \pi(assets/8e492feb468138fe89828a57fd00a832df6c934a.svg)](https://spinningup.openai.com/en/latest/_images/math/8e492feb468138fe89828a57fd00a832df6c934a.svg).

The Bellman equations for the optimal value functions are

![\begin{align*} V^*(assets/d68c87fb46a17130d25b040ad23fb2409ae764a1.svg) &= \max_a \underE{s'\sim P}{r(s,a) + \gamma V^*(s')}, \\ Q^*(s,a) &= \underE{s'\sim P}{r(s,a) + \gamma \max_{a'} Q^*(s',a')}. \end{align*}](https://spinningup.openai.com/en/latest/_images/math/d68c87fb46a17130d25b040ad23fb2409ae764a1.svg)

The crucial difference between the Bellman equations for the on-policy value functions and the optimal value functions, is the absence or presence of the ![\max](assets/f7d0d3d5adc0b470b1ce76ae6a4fdfe6d446d59b.svg)over actions. Its inclusion reflects the fact that whenever the agent gets to choose its action, in order to act optimally, it has to pick whichever action leads to the highest value.

> The term “Bellman backup” comes up quite frequently in the RL literature. The Bellman backup for a state, or state-action pair, is the right-hand side of the Bellman equation: the reward-plus-next-value.

#### Advantage Functions

Sometimes in RL, we don’t need to describe how good an action is in an absolute sense, but only how much better it is than others on average. That is to say, we want to know the relative **advantage** of that action. We make this concept precise with the **advantage function.**

The advantage function ![A^{\pi}(assets/ed4827d60a397e2d1dac0196a3ad178426554e64.svg)](https://spinningup.openai.com/en/latest/_images/math/ed4827d60a397e2d1dac0196a3ad178426554e64.svg) corresponding to a policy ![\pi](assets/b8a498bfdeca84b973d6222e08a6d04321d65dc3-20190613222156295.svg) describes how much better it is to take a specific action ![a](assets/045c129c396240f9c4f59bee864337cea3896ef0-20190613222156466.svg) in state ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309-20190613222156474.svg), over randomly selecting an action according to ![\pi(assets/e102d0f6f2fc70ae851589869125d5a1c031fd75.svg)](https://spinningup.openai.com/en/latest/_images/math/e102d0f6f2fc70ae851589869125d5a1c031fd75.svg), assuming you act according to ![\pi](assets/b8a498bfdeca84b973d6222e08a6d04321d65dc3-20190613222156295.svg) forever after. Mathematically, the advantage function is defined by

![A^{\pi}(assets/a596eb68ba26e424afaff142ae747d5cffd2be60.svg) = Q^{\pi}(s,a) - V^{\pi}(s).](https://spinningup.openai.com/en/latest/_images/math/a596eb68ba26e424afaff142ae747d5cffd2be60.svg)

## [(Optional) Formalism](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#id5)

So far, we’ve discussed the agent’s environment in an informal way, but if you try to go digging through the literature, you’re likely to run into the standard mathematical formalism for this setting: **Markov Decision Processes** (MDPs). An MDP is a 5-tuple, ![\langle S, A, R, P, \rho_0 \rangle](assets/677b3fbb988b8a8e6218c9be5c86de2be3ff4e67.svg), where

- ![S](assets/2ab5ab83469706ab8836e86f15564913d45841f3.svg) is the set of all valid states,
- ![A](assets/c65f5e15bab0ef7ff195cf8d2f4be7b3f88799c1.svg) is the set of all valid actions,
- ![R : S \times A \times S \to \mathbb{R}](assets/d6106e5c927964f012840fbdb2f6756e0478126f.svg) is the reward function, with ![r_t = R(assets/b53acbb815e813f805cd57b1ea15e6a497c8ddea.svg)](https://spinningup.openai.com/en/latest/_images/math/b53acbb815e813f805cd57b1ea15e6a497c8ddea.svg),
- ![P : S \times A \to \mathcal{P}(assets/3c87975dbb83cb5f033ed8a00f911eacb5593b79.svg)](https://spinningup.openai.com/en/latest/_images/math/3c87975dbb83cb5f033ed8a00f911eacb5593b79.svg) is the transition probability function, with ![P(assets/811e1959db4dd2c71d9806ee337d5014b96904df.svg)](https://spinningup.openai.com/en/latest/_images/math/811e1959db4dd2c71d9806ee337d5014b96904df.svg) being the probability of transitioning into state ![s'](assets/e571b6cfaee799ef1b1226dab6f2b72a093cd7a7-20190613222237897.svg) if you start in state ![s](assets/2f2cb3307c5fd6504354c54022ea6e82c9629309-20190613222237898.svg) and take action ![a](assets/045c129c396240f9c4f59bee864337cea3896ef0-20190613222237900.svg),
- and ![\rho_0](assets/b9c31eb35f400fd8e50b2d39bfe5698a50b9b6b5-20190613222237920.svg) is the starting state distribution.

The name Markov Decision Process refers to the fact that the system obeys the [Markov property](https://en.wikipedia.org/wiki/Markov_property): transitions only depend on the most recent state and action, and no prior history.





