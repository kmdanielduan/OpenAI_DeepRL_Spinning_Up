# OpenAI Spinning Up

## 3. Intro to Policy Optimization

URL: https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html

We will cover three key results in the theory of **policy gradients**:

- [the simplest equation](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient) describing the gradient of policy performance with respect to policy parameters,
- a rule which allows us to [drop useless terms](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you) from that expression,
- and a rule which allows us to [add useful terms](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#baselines-in-policy-gradients) to that expression.

In the end, we’ll tie those results together and describe the advantage-based expression for the policy gradient—the version we use in our [Vanilla Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/vpg.html) implementation.

### Deriving the Simplest Policy Gradient

Here, we consider the case of a stochastic, parameterized policy, ![\pi_{\theta}](assets/80088cfe6126980142c5447a9cb12f69ee7fa333.svg). We aim to maximize the expected return ![J(assets/48ffbf0dd0274a46574e145ea23e4c174f6dfaa3.svg) = \underE{\tau \sim \pi_{\theta}}{R(\tau)}](https://spinningup.openai.com/en/latest/_images/math/48ffbf0dd0274a46574e145ea23e4c174f6dfaa3.svg). For the purposes of this derivation, we’ll take ![R(assets/27905cd8b349a89eec129f7e3ac3d3ac4c82e501.svg)](https://spinningup.openai.com/en/latest/_images/math/27905cd8b349a89eec129f7e3ac3d3ac4c82e501.svg) to give the [finite-horizon undiscounted return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return), but the derivation for the infinite-horizon discounted return setting is almost identical.

We would like to optimize the policy by gradient ascent, eg

![\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(assets/237c86938ce2e9de91040e4090f79c6a1125fc00.svg) \right|_{\theta_k}.](https://spinningup.openai.com/en/latest/_images/math/237c86938ce2e9de91040e4090f79c6a1125fc00.svg)

The gradient of policy performance, ![\nabla_{\theta} J(assets/f839e53d1ccacb63d4d86a8be348d9adc86d723a.svg)](https://spinningup.openai.com/en/latest/_images/math/f839e53d1ccacb63d4d86a8be348d9adc86d723a.svg), is called the **policy gradient**, and algorithms that optimize the policy this way are called **policy gradient algorithms.** (Examples include Vanilla Policy Gradient and TRPO. PPO is often referred to as a policy gradient algorithm, though this is slightly inaccurate.)

To actually use this algorithm, we need an expression for the policy gradient which we can numerically compute. This involves two steps: 

1. deriving the analytical gradient of policy performance, which turns out to have the form of an expected value, and then
2. forming a sample estimate of that expected value, which can be computed with data from a finite number of agent-environment interaction steps.

In this subsection, we’ll find the simplest form of that expression. In later subsections, we’ll show how to improve on the simplest form to get the version we actually use in standard policy gradient implementations.

We’ll begin by laying out a few facts which are useful for deriving the analytical gradient.

**1. Probability of a Trajectory.** The probability of a trajectory ![\tau = (assets/8b650845f5290c4327f29ff073a5b1a8dabb7298.svg)](https://spinningup.openai.com/en/latest/_images/math/8b650845f5290c4327f29ff073a5b1a8dabb7298.svg) given that actions come from ![\pi_{\theta}](assets/80088cfe6126980142c5447a9cb12f69ee7fa333-20190617151745341.svg) is

![P(assets/2cb381fc364780ad05245f41cf88a9467bf4cb2f.svg) = \rho_0 (s_0) \prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \pi_{\theta}(a_t |s_t).](https://spinningup.openai.com/en/latest/_images/math/2cb381fc364780ad05245f41cf88a9467bf4cb2f.svg)

**2. The Log-Derivative Trick.** The log-derivative trick is based on a simple rule from calculus: the derivative of ![\log x](assets/ef040455c459db2f570c11ecb75b25853645d90a.svg) with respect to ![x](assets/a679a9634cd3d168023caa7576bcbcbaeaa0d7d5.svg) is ![1/x](assets/4a5449dd5e4be3a804c40210a24c5154bcd6760e.svg). When rearranged and combined with chain rule, we get:

![\nabla_{\theta} P(assets/2f10287db9a459af5467140025c35bb92e960ee3.svg) = P(\tau | \theta) \nabla_{\theta} \log P(\tau | \theta).](https://spinningup.openai.com/en/latest/_images/math/2f10287db9a459af5467140025c35bb92e960ee3.svg)

**3. Log-Probability of a Trajectory.** The log-prob of a trajectory is just

![\log P(assets/1737476897d122c10cec3053f4d360f8b57d0a01.svg) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \bigg( \log P(s_{t+1}|s_t, a_t)  + \log \pi_{\theta}(a_t |s_t)\bigg).](https://spinningup.openai.com/en/latest/_images/math/1737476897d122c10cec3053f4d360f8b57d0a01.svg)

**4. Gradients of Environment Functions.** The environment has no dependence on ![\theta](assets/1a20bd03ccceae216a40cb69d3fb7a3970f6f275-0757921.svg), so gradients of ![\rho_0(assets/b6ef65214189bd57d5130c050c9848928f697389.svg)](https://spinningup.openai.com/en/latest/_images/math/b6ef65214189bd57d5130c050c9848928f697389.svg), ![P(assets/acce6f3cb29fba6f467260234cf1ad858c53822d.svg)](https://spinningup.openai.com/en/latest/_images/math/acce6f3cb29fba6f467260234cf1ad858c53822d.svg), and ![R(assets/27905cd8b349a89eec129f7e3ac3d3ac4c82e501-20190617155201606.svg)](https://spinningup.openai.com/en/latest/_images/math/27905cd8b349a89eec129f7e3ac3d3ac4c82e501.svg) are zero.

**5. Grad-Log-Prob of a Trajectory.** The gradient of the log-prob of a trajectory is thus

![\nabla_{\theta} \log P(assets/5661deae547ee000037bc5ecc5e3077de6ee57db.svg) &= \cancel{\nabla_{\theta} \log \rho_0 (s_0)} + \sum_{t=0}^{T} \bigg( \cancel{\nabla_{\theta} \log P(s_{t+1}|s_t, a_t)}  + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t)\bigg) \\ &= \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t).](https://spinningup.openai.com/en/latest/_images/math/5661deae547ee000037bc5ecc5e3077de6ee57db.svg)

**Derivation for Basic Policy Gradient**

![6fcf142138ce8289bb4f5d4656f3f3bf1609214d](assets/6fcf142138ce8289bb4f5d4656f3f3bf1609214d.svg)

This is an expectation, which means that we can estimate it with a sample mean. If we collect a set of trajectories ![\mathcal{D} = \{\tau_i\}_{i=1,...,N}](assets/2618cb22cf47efc780221f34206472f7bf678097.svg) where each trajectory is obtained by letting the agent act in the environment using the policy ![\pi_{\theta}](assets/80088cfe6126980142c5447a9cb12f69ee7fa333-20190617160120554.svg), the policy gradient can be estimated with

![\hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(assets/a8ec906d99c7cb540ef0df80d86fa1bca0f33a79.svg) R(\tau),](https://spinningup.openai.com/en/latest/_images/math/a8ec906d99c7cb540ef0df80d86fa1bca0f33a79.svg)

where ![|\mathcal{D}|](assets/1889e2b0692b42dc0ec4ea24fbbfa98745ac198f.svg) is the number of trajectories in ![\mathcal{D}](assets/b8fda719f4eca2e94bdde65ed40504645326e5fd.svg) (here, ![N](assets/bc31bc706968058d05385235d40e5be6449ce21a.svg)).

This last expression is the simplest version of the computable expression we desired. Assuming that we have represented our policy in a way which allows us to calculate ![\nabla_{\theta} \log \pi_{\theta}(assets/dcddd87f44c485d068a897bcdcd421893db63db3.svg)](https://spinningup.openai.com/en/latest/_images/math/dcddd87f44c485d068a897bcdcd421893db63db3.svg), and if we are able to run the policy in the environment to collect the trajectory dataset, we can compute the policy gradient and take an update step.

### Implementing the Simplest Policy Gradient

We give a short Tensorflow implementation of this simple version of the policy gradient algorithm in `spinup/examples/pg_math/1_simple_pg.py`. (It can also be viewed [on github](https://github.com/openai/spinningup/blob/master/spinup/examples/pg_math/1_simple_pg.py).) It is only 122 lines long, so we highly recommend reading through it in depth. While we won’t go through the entirety of the code here, we’ll highlight and explain a few important pieces.

**1. Making the Policy Network.**

```python
# make core of policy network
obs_ph = tf.placeholder(shape=(None, obs_dim),dtype=tf.float32)
logits = mlp(obs_ph, sizes=hidden_sizes+[n_acts])

# make action selection op (outputs int actions, sampled from policy)
actions = tf.squeeze(tf.multinomial(logits=logits, num_samples=1), axis=1)
```

This block builds a feedforward neural network categorical policy. (See the [Stochastic Policies](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies) section in Part 1 for a refresher.) The `logits` tensor can be used to construct log-probabilities and probabilities for actions, and the `actions` tensor samples actions based on the probabilities implied by `logits`.

**2. Making the Loss Function.**

```python
# make loss function whose gradient, for the right data, is policy gradient
weights_ph = tf.placeholder(shape=(None,), dtype=tf.float32)
act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)
action_masks = tf.one_hot(act_ph, n_acts)
log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)
loss = -tf.reduce_mean(weights_ph * log_probs)
```

> **1. The data distribution depends on the parameters.** A loss function is usually defined on a fixed data distribution which is independent of the parameters we aim to optimize. Not so here, where the data must be sampled on the most recent policy.
>
> **2. It doesn’t measure performance.** A loss function usually evaluates the performance metric that we care about. Here, we care about expected return, ![J(assets/bac2be784363161f5e76c206e3319aa035c066fc-0824001.svg)](https://spinningup.openai.com/en/latest/_images/math/bac2be784363161f5e76c206e3319aa035c066fc.svg), but our “loss” function does not approximate this at all, even in expectation. This “loss” function is only useful to us because, when evaluated at the current parameters, with data generated by the current parameters, it has the negative gradient of performance.
>
> But after that first step of gradient descent, there is no more connection to performance. This means that minimizing this “loss” function, for a given batch of data, has *no* guarantee whatsoever of improving expected return. You can send this loss to ![-\infty](assets/5b06f1fe8ec90d14f7597c7bf106758a9fed48f8.svg) and policy performance could crater; in fact, it usually will. Sometimes a deep RL researcher might describe this outcome as the policy “overfitting” to a batch of data. This is descriptive, but should not be taken literally because it does not refer to generalization error.
>
> We raise this point because it is common for ML practitioners to interpret a loss function as a useful signal during training—”if the loss goes down, all is well.” In policy gradients, this intuition is wrong, and you should only care about average return. The loss function means nothing.

The approach used here to make the `log_probs` tensor—creating an action mask, and using it to select out particular log probabilities—*only* works for categorical policies. It does not work in general.

**3. Running One Epoch of Training.**

```python
  # for training policy
    def train_one_epoch():
        # make some empty lists for logging.
        batch_obs = []          # for observations
        batch_acts = []         # for actions
        batch_weights = []      # for R(tau) weighting in policy gradient
        batch_rets = []         # for measuring episode returns
        batch_lens = []         # for measuring episode lengths

        # reset episode-specific variables
        obs = env.reset()       # first obs comes from starting distribution
        done = False            # signal from environment that episode is over
        ep_rews = []            # list for rewards accrued throughout ep

        # render first episode of each epoch
        finished_rendering_this_epoch = False

        # collect experience by acting in the environment with current policy
        while True:

            # rendering
            if not(finished_rendering_this_epoch):
                env.render()

            # save obs
            batch_obs.append(obs.copy())

            # act in the environment
            act = sess.run(actions, {obs_ph: obs.reshape(1,-1)})[0]
            obs, rew, done, _ = env.step(act)

            # save action, reward
            batch_acts.append(act)
            ep_rews.append(rew)

            if done:
                # if episode is over, record info about episode
                ep_ret, ep_len = sum(ep_rews), len(ep_rews)
                batch_rets.append(ep_ret)
                batch_lens.append(ep_len)

                # the weight for each logprob(a|s) is R(tau)
                batch_weights += [ep_ret] * ep_len

                # reset episode-specific variables
                obs, done, ep_rews = env.reset(), False, []

                # won't render again this epoch
                finished_rendering_this_epoch = True

                # end experience loop if we have enough of it
                if len(batch_obs) > batch_size:
                    break

        # take a single policy gradient update step
        batch_loss, _ = sess.run([loss, train_op],
                                 feed_dict={
                                    obs_ph: np.array(batch_obs),
                                    act_ph: np.array(batch_acts),
                                    weights_ph: np.array(batch_weights)
                                 })
        return batch_loss, batch_rets, batch_lens
```

The `train_one_epoch()` function runs one “epoch” of policy gradient, which we define to be

1. the experience collection step (L62-97), where the agent acts for some number of episodes in the environment using the most recent policy, followed by
2. a single policy gradient update step (L99-105).

The main loop of the algorithm just repeatedly calls `train_one_epoch()`.

### Expected Grad-Log-Prob Lemma

In this subsection, we will derive an intermediate result which is extensively used throughout the theory of policy gradients. We will call it the Expected Grad-Log-Prob (EGLP) lemma.

**EGLP Lemma.** Suppose that ![P_{\theta}](assets/16b92ba11837118ba08f3bf14c65e6151a9def9a.svg) is a parameterized probability distribution over a random variable, ![x](assets/a679a9634cd3d168023caa7576bcbcbaeaa0d7d5-0824139.svg). Then:

![\underE{x \sim P_{\theta}}{\nabla_{\theta} \log P_{\theta}(assets/458b0eb0829ecd27ff745f9329fdc0fbd56295bf.svg)} = 0.](https://spinningup.openai.com/en/latest/_images/math/458b0eb0829ecd27ff745f9329fdc0fbd56295bf.svg)

Recall that all probability distributions are *normalized*:

![\int_x P_{\theta}(assets/1ccff802b56424217fbdb65469e70a2780a676e2.svg) = 1.](https://spinningup.openai.com/en/latest/_images/math/1ccff802b56424217fbdb65469e70a2780a676e2.svg)

Take the gradient of both sides of the normalization condition:

![\nabla_{\theta} \int_x P_{\theta}(assets/e9271f247e0b161843ba47329c92cd43e7395a7f.svg) = \nabla_{\theta} 1 = 0.](https://spinningup.openai.com/en/latest/_images/math/e9271f247e0b161843ba47329c92cd43e7395a7f.svg)

Use the log derivative trick to get:

![0 &= \nabla_{\theta} \int_x P_{\theta}(assets/2f52c5df9b4abb4b165f4e4a938d877d6441c2d3.svg) \\ &= \int_x \nabla_{\theta} P_{\theta}(x) \\ &= \int_x P_{\theta}(x) \nabla_{\theta} \log P_{\theta}(x) \\ \therefore 0 &= \underE{x \sim P_{\theta}}{\nabla_{\theta} \log P_{\theta}(x)}.](https://spinningup.openai.com/en/latest/_images/math/2f52c5df9b4abb4b165f4e4a938d877d6441c2d3.svg)

### Don’t Let the Past Distract You

Examine our most recent expression for the policy gradient:

![\nabla_{\theta} J(assets/777ef86f94711dca46888226a5f5dd9ee40811db.svg) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)}.](https://spinningup.openai.com/en/latest/_images/math/777ef86f94711dca46888226a5f5dd9ee40811db.svg)

Taking a step with this gradient pushes up the log-probabilities of each action in proportion to ![R(assets/27905cd8b349a89eec129f7e3ac3d3ac4c82e501-0824376.svg)](https://spinningup.openai.com/en/latest/_images/math/27905cd8b349a89eec129f7e3ac3d3ac4c82e501.svg), the sum of *all rewards ever obtained*. But this doesn’t make much sense.

Agents should really only reinforce actions on the basis of their *consequences*. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come *after*.

It turns out that this intuition shows up in the math, and we can show that the policy gradient can also be expressed by

![\nabla_{\theta} J(assets/9926209f608ff0134af57f8b5fa4ecf0ea515480.svg) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})}.](https://spinningup.openai.com/en/latest/_images/math/9926209f608ff0134af57f8b5fa4ecf0ea515480.svg)

In this form, actions are only reinforced based on rewards obtained after they are taken.

We’ll call this form the “reward-to-go policy gradient,” because the sum of rewards after a point in a trajectory,

![\hat{R}_t \doteq \sum_{t'=t}^T R(assets/1bccba32148775ea311d8c14933c6232e0828315.svg),](https://spinningup.openai.com/en/latest/_images/math/1bccba32148775ea311d8c14933c6232e0828315.svg)

is called the **reward-to-go** from that point, and this policy gradient expression depends on the reward-to-go from state-action pairs.

> **But how is this better?** A key problem with policy gradients is how many sample trajectories are needed to get a low-variance sample estimate for them. The formula we started with included terms for reinforcing actions proportional to past rewards, all of which had zero mean, but nonzero variance: as a result, they would just add noise to sample estimates of the policy gradient. By removing them, we reduce the number of sample trajectories needed.

### Implementing Reward-to-Go Policy Gradient

The only thing that has changed from `1_simple_pg.py` is that we now use different weights in the loss function. The code modification is very slight: we add a new function, and change two other lines. The new function is:

```python
def reward_to_go(rews):
    n = len(rews)
    rtgs = np.zeros_like(rews)
    for i in reversed(range(n)):
        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)
    return rtgs
```

And then we tweak the old L86-87 from:

```python
                # the weight for each logprob(a|s) is R(tau)
                batch_weights += [ep_ret] * ep_len
```

to:

```python
                # the weight for each logprob(a_t|s_t) is reward-to-go from t
                batch_weights += list(reward_to_go(ep_rews))
```

### Baselines in Policy Gradients

An immediate consequence of the EGLP lemma is that for any function ![b](assets/39198dc626d803f8b1266d70dabb59b1ac38b6d2.svg) which only depends on state,

![\underE{a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(assets/18738f8112fa4138e032a4be06e898543e587346.svg) b(s_t)} = 0.](https://spinningup.openai.com/en/latest/_images/math/18738f8112fa4138e032a4be06e898543e587346.svg)

This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:

![\nabla_{\theta} J(assets/a68f53bd1391212d17d426dafeb71a39c10c9189.svg) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \left(\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t)\right)}.](https://spinningup.openai.com/en/latest/_images/math/a68f53bd1391212d17d426dafeb71a39c10c9189.svg)

Any function ![b](assets/39198dc626d803f8b1266d70dabb59b1ac38b6d2-20190618102655747.svg)used in this way is called a **baseline**.

The most common choice of baseline is the [on-policy value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions) ![V^{\pi}(assets/d1a28691e5690a9f6b7bce161b03bd1fc8eadbd8.svg)](https://spinningup.openai.com/en/latest/_images/math/d1a28691e5690a9f6b7bce161b03bd1fc8eadbd8.svg). Recall that this is the average return an agent gets if it starts in state ![s_t](assets/8b415ee5fdf45796473d9a05615664c67e20af3d-0824878.svg) and then acts according to policy ![\pi](assets/b8a498bfdeca84b973d6222e08a6d04321d65dc3-0824878.svg) for the rest of its life.

Empirically, the choice ![b(assets/2221a29de2953ebb8930423425ed4d0feea27b25.svg) = V^{\pi}(s_t)](https://spinningup.openai.com/en/latest/_images/math/2221a29de2953ebb8930423425ed4d0feea27b25.svg) has the desirable effect of reducing variance in the sample estimate for the policy gradient. This results in faster and more stable policy learning. It is also appealing from a conceptual angle: it encodes the intuition that if an agent gets what it expected, it should “feel” neutral about it.

In practice, ![V^{\pi}(assets/d1a28691e5690a9f6b7bce161b03bd1fc8eadbd8-20190618102837554.svg)](https://spinningup.openai.com/en/latest/_images/math/d1a28691e5690a9f6b7bce161b03bd1fc8eadbd8.svg) cannot be computed exactly, so it has to be approximated. This is usually done with a neural network, ![V_{\phi}(assets/a133925364fc62de281e792d4a41fbc9c360967f.svg)](https://spinningup.openai.com/en/latest/_images/math/a133925364fc62de281e792d4a41fbc9c360967f.svg), which is updated concurrently with the policy (so that the value network always approximates the value function of the most recent policy).

The simplest method for learning ![V_{\phi}](assets/662f11bc664c365835c552ff09b43fc4bc88f721.svg), used in most implementations of policy optimization algorithms (including VPG, TRPO, PPO, and A2C), is to minimize a mean-squared-error objective:

![\phi_k = \arg \min_{\phi} \underE{s_t, \hat{R}_t \sim \pi_k}{\left(assets/e72767ea0e9d0fba2e5ab7af60aad01af4cdf0e0.svg)^2},](https://spinningup.openai.com/en/latest/_images/math/e72767ea0e9d0fba2e5ab7af60aad01af4cdf0e0.svg)



where ![\pi_k](assets/9a2555f9b7533949f38b997c919bf86862ec0fae.svg) is the policy at epoch ![k](assets/ce641c655353b854e8d2ae836c6f97df9ace9984-0824917.svg). This is done with one or more steps of gradient descent, starting from the previous value parameters ![\phi_{k-1}](assets/7681f537ec31c9bc1c811dd0f33f46aa9aaabebf.svg).

### Other Forms of the Policy Gradient

What we have seen so far is that the policy gradient has the general form

![\nabla_{\theta} J(assets/1485ca5baaa09ed99fbcc54ba600e36852afd36c.svg) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Phi_t},](https://spinningup.openai.com/en/latest/_images/math/1485ca5baaa09ed99fbcc54ba600e36852afd36c.svg)

where ![\Phi_t](assets/aaba82ffd0d44c6c1a14a125c83424b1a63ca7fb.svg) could be any of

![\Phi_t &= R(assets/d728e9e6107056e4411a184d1a57f1510f79fa7d.svg),](https://spinningup.openai.com/en/latest/_images/math/d728e9e6107056e4411a184d1a57f1510f79fa7d.svg)

or

![\Phi_t &= \sum_{t'=t}^T R(assets/c9dd451211b3afb0bc9dc0c19d477b23e931598a.svg),](https://spinningup.openai.com/en/latest/_images/math/c9dd451211b3afb0bc9dc0c19d477b23e931598a.svg)

or

![\Phi_t &= \sum_{t'=t}^T R(assets/92ea8688d614735fd59cf9ecd9a36672ab5d7def.svg) - b(s_t).](https://spinningup.openai.com/en/latest/_images/math/92ea8688d614735fd59cf9ecd9a36672ab5d7def.svg)

All of these choices lead to the same expected value for the policy gradient, despite having different variances. It turns out that there are two more valid choices of weights ![aaba82ffd0d44c6c1a14a125c83424b1a63ca7fb](assets/aaba82ffd0d44c6c1a14a125c83424b1a63ca7fb-20190618102943819.svg) which are important to know.

**1. On-Policy Action-Value Function.** The choice

![\Phi_t = Q^{\pi_{\theta}}(assets/bf6f5680c7568790c744baf59bbc27831603f200.svg)](https://spinningup.openai.com/en/latest/_images/math/bf6f5680c7568790c744baf59bbc27831603f200.svg)

is also valid. See [this page](https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof2.html) for an (optional) proof of this claim.

**2. The Advantage Function.** Recall that the [advantage of an action](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), defined by ![A^{\pi}(assets/b01b7882de374f84fd2686fab209b641d8e7277b.svg) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)](https://spinningup.openai.com/en/latest/_images/math/b01b7882de374f84fd2686fab209b641d8e7277b.svg), describes how much better or worse it is than other actions on average (relative to the current policy). This choice,

![\Phi_t = A^{\pi_{\theta}}(assets/06e42f4a5a133c3a56d70aaa098c23c3f0a37df2.svg)](https://spinningup.openai.com/en/latest/_images/math/06e42f4a5a133c3a56d70aaa098c23c3f0a37df2.svg)

is also valid. The proof is that it’s equivalent to using ![\Phi_t = Q^{\pi_{\theta}}(assets/2e609266200dc3d0c82e592e1be4755f6e3c564c.svg)](https://spinningup.openai.com/en/latest/_images/math/2e609266200dc3d0c82e592e1be4755f6e3c564c.svg) and then using a value function baseline, which we are always free to do.

