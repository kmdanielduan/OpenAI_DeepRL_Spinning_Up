# OpenAI Spinning Up

## 3. Intro to Policy Optimization

URL: https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html

We will cover three key results in the theory of **policy gradients**:

- [the simplest equation](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient) describing the gradient of policy performance with respect to policy parameters,
- a rule which allows us to [drop useless terms](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you) from that expression,
- and a rule which allows us to [add useful terms](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#baselines-in-policy-gradients) to that expression.

In the end, we’ll tie those results together and describe the advantage-based expression for the policy gradient—the version we use in our [Vanilla Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/vpg.html) implementation.

### Deriving the Simplest Policy Gradient

Here, we consider the case of a stochastic, parameterized policy, ![\pi_{\theta}](assets/80088cfe6126980142c5447a9cb12f69ee7fa333.svg). We aim to maximize the expected return ![J(assets/48ffbf0dd0274a46574e145ea23e4c174f6dfaa3.svg) = \underE{\tau \sim \pi_{\theta}}{R(\tau)}](https://spinningup.openai.com/en/latest/_images/math/48ffbf0dd0274a46574e145ea23e4c174f6dfaa3.svg). For the purposes of this derivation, we’ll take ![R(assets/27905cd8b349a89eec129f7e3ac3d3ac4c82e501.svg)](https://spinningup.openai.com/en/latest/_images/math/27905cd8b349a89eec129f7e3ac3d3ac4c82e501.svg) to give the [finite-horizon undiscounted return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return), but the derivation for the infinite-horizon discounted return setting is almost identical.

We would like to optimize the policy by gradient ascent, eg

![\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(assets/237c86938ce2e9de91040e4090f79c6a1125fc00.svg) \right|_{\theta_k}.](https://spinningup.openai.com/en/latest/_images/math/237c86938ce2e9de91040e4090f79c6a1125fc00.svg)

The gradient of policy performance, ![\nabla_{\theta} J(assets/f839e53d1ccacb63d4d86a8be348d9adc86d723a.svg)](https://spinningup.openai.com/en/latest/_images/math/f839e53d1ccacb63d4d86a8be348d9adc86d723a.svg), is called the **policy gradient**, and algorithms that optimize the policy this way are called **policy gradient algorithms.** (Examples include Vanilla Policy Gradient and TRPO. PPO is often referred to as a policy gradient algorithm, though this is slightly inaccurate.)

To actually use this algorithm, we need an expression for the policy gradient which we can numerically compute. This involves two steps: 

1. deriving the analytical gradient of policy performance, which turns out to have the form of an expected value, and then
2. forming a sample estimate of that expected value, which can be computed with data from a finite number of agent-environment interaction steps.

In this subsection, we’ll find the simplest form of that expression. In later subsections, we’ll show how to improve on the simplest form to get the version we actually use in standard policy gradient implementations.

We’ll begin by laying out a few facts which are useful for deriving the analytical gradient.

**1. Probability of a Trajectory.** The probability of a trajectory ![\tau = (assets/8b650845f5290c4327f29ff073a5b1a8dabb7298.svg)](https://spinningup.openai.com/en/latest/_images/math/8b650845f5290c4327f29ff073a5b1a8dabb7298.svg) given that actions come from ![\pi_{\theta}](assets/80088cfe6126980142c5447a9cb12f69ee7fa333-20190617151745341.svg) is

![P(assets/2cb381fc364780ad05245f41cf88a9467bf4cb2f.svg) = \rho_0 (s_0) \prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \pi_{\theta}(a_t |s_t).](https://spinningup.openai.com/en/latest/_images/math/2cb381fc364780ad05245f41cf88a9467bf4cb2f.svg)

**2. The Log-Derivative Trick.** The log-derivative trick is based on a simple rule from calculus: the derivative of ![\log x](assets/ef040455c459db2f570c11ecb75b25853645d90a.svg) with respect to ![x](assets/a679a9634cd3d168023caa7576bcbcbaeaa0d7d5.svg) is ![1/x](assets/4a5449dd5e4be3a804c40210a24c5154bcd6760e.svg). When rearranged and combined with chain rule, we get:

![\nabla_{\theta} P(assets/2f10287db9a459af5467140025c35bb92e960ee3.svg) = P(\tau | \theta) \nabla_{\theta} \log P(\tau | \theta).](https://spinningup.openai.com/en/latest/_images/math/2f10287db9a459af5467140025c35bb92e960ee3.svg)

**3. Log-Probability of a Trajectory.** The log-prob of a trajectory is just

![\log P(assets/1737476897d122c10cec3053f4d360f8b57d0a01.svg) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \bigg( \log P(s_{t+1}|s_t, a_t)  + \log \pi_{\theta}(a_t |s_t)\bigg).](https://spinningup.openai.com/en/latest/_images/math/1737476897d122c10cec3053f4d360f8b57d0a01.svg)

**4. Gradients of Environment Functions.** The environment has no dependence on ![\theta](assets/1a20bd03ccceae216a40cb69d3fb7a3970f6f275-0757921.svg), so gradients of ![\rho_0(assets/b6ef65214189bd57d5130c050c9848928f697389.svg)](https://spinningup.openai.com/en/latest/_images/math/b6ef65214189bd57d5130c050c9848928f697389.svg), ![P(assets/acce6f3cb29fba6f467260234cf1ad858c53822d.svg)](https://spinningup.openai.com/en/latest/_images/math/acce6f3cb29fba6f467260234cf1ad858c53822d.svg), and ![R(assets/27905cd8b349a89eec129f7e3ac3d3ac4c82e501-20190617155201606.svg)](https://spinningup.openai.com/en/latest/_images/math/27905cd8b349a89eec129f7e3ac3d3ac4c82e501.svg) are zero.

**5. Grad-Log-Prob of a Trajectory.** The gradient of the log-prob of a trajectory is thus

![\nabla_{\theta} \log P(assets/5661deae547ee000037bc5ecc5e3077de6ee57db.svg) &= \cancel{\nabla_{\theta} \log \rho_0 (s_0)} + \sum_{t=0}^{T} \bigg( \cancel{\nabla_{\theta} \log P(s_{t+1}|s_t, a_t)}  + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t)\bigg) \\ &= \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t).](https://spinningup.openai.com/en/latest/_images/math/5661deae547ee000037bc5ecc5e3077de6ee57db.svg)

**Derivation for Basic Policy Gradient**

![6fcf142138ce8289bb4f5d4656f3f3bf1609214d](assets/6fcf142138ce8289bb4f5d4656f3f3bf1609214d.svg)

This is an expectation, which means that we can estimate it with a sample mean. If we collect a set of trajectories ![\mathcal{D} = \{\tau_i\}_{i=1,...,N}](assets/2618cb22cf47efc780221f34206472f7bf678097.svg) where each trajectory is obtained by letting the agent act in the environment using the policy ![\pi_{\theta}](assets/80088cfe6126980142c5447a9cb12f69ee7fa333-20190617160120554.svg), the policy gradient can be estimated with

![\hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(assets/a8ec906d99c7cb540ef0df80d86fa1bca0f33a79.svg) R(\tau),](https://spinningup.openai.com/en/latest/_images/math/a8ec906d99c7cb540ef0df80d86fa1bca0f33a79.svg)

where ![|\mathcal{D}|](assets/1889e2b0692b42dc0ec4ea24fbbfa98745ac198f.svg) is the number of trajectories in ![\mathcal{D}](assets/b8fda719f4eca2e94bdde65ed40504645326e5fd.svg) (here, ![N](assets/bc31bc706968058d05385235d40e5be6449ce21a.svg)).

This last expression is the simplest version of the computable expression we desired. Assuming that we have represented our policy in a way which allows us to calculate ![\nabla_{\theta} \log \pi_{\theta}(assets/dcddd87f44c485d068a897bcdcd421893db63db3.svg)](https://spinningup.openai.com/en/latest/_images/math/dcddd87f44c485d068a897bcdcd421893db63db3.svg), and if we are able to run the policy in the environment to collect the trajectory dataset, we can compute the policy gradient and take an update step.

### Implementing the Simplest Policy Gradient

We give a short Tensorflow implementation of this simple version of the policy gradient algorithm in `spinup/examples/pg_math/1_simple_pg.py`. (It can also be viewed [on github](https://github.com/openai/spinningup/blob/master/spinup/examples/pg_math/1_simple_pg.py).) It is only 122 lines long, so we highly recommend reading through it in depth. While we won’t go through the entirety of the code here, we’ll highlight and explain a few important pieces.

**1. Making the Policy Network.**

```python
# make core of policy network
obs_ph = tf.placeholder(shape=(None, obs_dim),dtype=tf.float32)
logits = mlp(obs_ph, sizes=hidden_sizes+[n_acts])

# make action selection op (outputs int actions, sampled from policy)
actions = tf.squeeze(tf.multinomial(logits=logits, num_samples=1), axis=1)
```

This block builds a feedforward neural network categorical policy. (See the [Stochastic Policies](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies) section in Part 1 for a refresher.) The `logits` tensor can be used to construct log-probabilities and probabilities for actions, and the `actions` tensor samples actions based on the probabilities implied by `logits`.

**2. Making the Loss Function.**

```python
# make loss function whose gradient, for the right data, is policy gradient
weights_ph = tf.placeholder(shape=(None,), dtype=tf.float32)
act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)
action_masks = tf.one_hot(act_ph, n_acts)
log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)
loss = -tf.reduce_mean(weights_ph * log_probs)
```







